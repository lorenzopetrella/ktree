{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "fcnn.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "mount_file_id": "1yPa4_TERRnpMOCLciJNiJVyLEDfyy26l",
   "authorship_tag": "ABX9TyOaOg470G1N56mzXs8ChE/z"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Set according to environment (e.g. local, Google Colab...)**"
   ],
   "metadata": {
    "id": "lFrTiHVXnp5S"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "project_folder = ''"
   ],
   "metadata": {
    "id": "R1TynGfJn9FM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1647888135726,
     "user_tz": -60,
     "elapsed": 4,
     "user": {
      "displayName": "Lorenzo Petrella",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15951970565465703880"
     }
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Body**"
   ],
   "metadata": {
    "id": "iZFxvzHy_pb6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from custom_libraries.image_dataset import *\n",
    "from custom_libraries.miscellaneous import *\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def create_model(num_units):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(units=num_units, activation=None, kernel_initializer=tf.keras.initializers.HeNormal))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=.01))\n",
    "    model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "id": "GJEAI7dJBKGc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1647888140358,
     "user_tz": -60,
     "elapsed": 4,
     "user": {
      "displayName": "Lorenzo Petrella",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15951970565465703880"
     }
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "bs = 256\n",
    "trials = 10\n",
    "epochs = 2000\n",
    "trees_set = [1, 2, 4, 8, 16, 32]\n",
    "\n",
    "classes = np.load(project_folder + 'results/classes.npy', allow_pickle=True)\n",
    "# TODO: verificare l'effetto di patience sul risultato di svhn (rischio underfitting)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=60)\n",
    "\n",
    "history = np.zeros((len(classes), trials, len(trees_set), 2))\n",
    "\n",
    "#history = np.load(project_folder+'results/fcnn_history.npy', allow_pickle=True)\n",
    "#history[-2,:,:,:] = 0\n",
    "\n",
    "for j, (t1, t2, ds) in enumerate(classes):\n",
    "\n",
    "    print(f\"Dataset: {ds} / Pair: {t1}-{t2}\")\n",
    "\n",
    "    if history[j, 0, 0, 0] != 0:\n",
    "        continue\n",
    "\n",
    "    test_ds = ImageDataset(ds, 'test', USPS_dir=project_folder + 'USPS/')\n",
    "    train_ds = ImageDataset(ds, 'train', USPS_dir=project_folder + 'USPS/')\n",
    "\n",
    "    for x in [train_ds, test_ds]:\n",
    "        x.filter(t1, t2, overwrite=True)\n",
    "        x.shuffle()\n",
    "        x.normalize()\n",
    "        if x.images.shape[1:3] == (28, 28):\n",
    "            x.pad()\n",
    "        x.vectorize(True)\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid = train_ds.subset(shard=True, shard_number=trials, validation=True,\n",
    "                                                         validation_size=len(test_ds.images))\n",
    "    test_set = tf.data.Dataset.from_tensor_slices((test_ds.images, test_ds.labels)).batch(bs)\n",
    "\n",
    "    for k, trees in enumerate(trees_set):\n",
    "\n",
    "        print(f\"{trees}-FCNN\")\n",
    "\n",
    "        for i in range(trials):\n",
    "\n",
    "            if history[j, i, k, 0] != 0:\n",
    "                continue\n",
    "\n",
    "            print(f\"Trial {i + 1}\")\n",
    "\n",
    "            model = create_model(num_units=2 * trees)\n",
    "\n",
    "            train_set = tf.data.Dataset.from_tensor_slices((X_train[i], y_train[i])).batch(bs)\n",
    "            valid_set = tf.data.Dataset.from_tensor_slices((X_valid[i], y_valid[i])).batch(bs)\n",
    "\n",
    "            with tf.device('/device:GPU:0'):\n",
    "\n",
    "                fit_history = model.fit(x=train_set, batch_size=bs, epochs=epochs,\n",
    "                                        validation_data=valid_set, validation_batch_size=bs,\n",
    "                                        callbacks=[callback], verbose=0)\n",
    "                print_fit_history(fit_history, epochs)\n",
    "\n",
    "                evaluate_history = model.evaluate(x=test_set, batch_size=bs, verbose=0)\n",
    "                print_evaluate_history(evaluate_history)\n",
    "\n",
    "                history[j, i, k] = evaluate_history\n",
    "\n",
    "                np.save(project_folder + 'results/fcnn_history.npy', history,\n",
    "                        allow_pickle=True)"
   ],
   "metadata": {
    "id": "_p5yaGWI_rhQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1647890584560,
     "user_tz": -60,
     "elapsed": 2444205,
     "user": {
      "displayName": "Lorenzo Petrella",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15951970565465703880"
     }
    },
    "outputId": "70219fc4-624b-4bae-ee8d-729dac5ceda3"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset: mnist / Pair: 3-5\n",
      "Dataset: fmnist / Pair: 0-6\n",
      "Dataset: kmnist / Pair: 2-6\n",
      "Dataset: emnist / Pair: 14-17\n",
      "Dataset: svhn / Pair: 5-6\n",
      "Dataset: usps / Pair: 3-5\n",
      "1-FCNN\n",
      "Trial 1\n",
      "Epochs: 2000/2000 - Train loss: 35.93%, accuracy: 95.51% - Validation loss: 45.42%, accuracy: 78.79%\n",
      "Test loss: 44.72%, accuracy: 83.44%\n",
      "Trial 2\n",
      "Epochs: 2000/2000 - Train loss: 38.47%, accuracy: 97.75% - Validation loss: 40.03%, accuracy: 90.91%\n",
      "Test loss: 43.3%, accuracy: 88.96%\n",
      "Trial 3\n",
      "Epochs: 2000/2000 - Train loss: 38.75%, accuracy: 100.0% - Validation loss: 43.4%, accuracy: 87.88%\n",
      "Test loss: 45.52%, accuracy: 86.2%\n",
      "Trial 4\n",
      "Epochs: 2000/2000 - Train loss: 40.54%, accuracy: 94.38% - Validation loss: 46.57%, accuracy: 84.85%\n",
      "Test loss: 43.23%, accuracy: 89.57%\n",
      "Trial 5\n",
      "Epochs: 2000/2000 - Train loss: 34.75%, accuracy: 95.51% - Validation loss: 43.32%, accuracy: 87.88%\n",
      "Test loss: 48.42%, accuracy: 79.45%\n",
      "Trial 6\n",
      "Epochs: 2000/2000 - Train loss: 39.11%, accuracy: 89.89% - Validation loss: 45.08%, accuracy: 78.79%\n",
      "Test loss: 50.1%, accuracy: 76.69%\n",
      "Trial 7\n",
      "Epochs: 2000/2000 - Train loss: 50.36%, accuracy: 91.01% - Validation loss: 53.01%, accuracy: 87.5%\n",
      "Test loss: 54.12%, accuracy: 84.36%\n",
      "Trial 8\n",
      "Epochs: 2000/2000 - Train loss: 38.74%, accuracy: 92.13% - Validation loss: 48.74%, accuracy: 84.38%\n",
      "Test loss: 45.99%, accuracy: 88.04%\n",
      "Trial 9\n",
      "Epochs: 2000/2000 - Train loss: 39.57%, accuracy: 96.59% - Validation loss: 35.79%, accuracy: 100.0%\n",
      "Test loss: 43.56%, accuracy: 88.96%\n",
      "Trial 10\n",
      "Epochs: 2000/2000 - Train loss: 36.46%, accuracy: 92.05% - Validation loss: 37.74%, accuracy: 90.62%\n",
      "Test loss: 46.63%, accuracy: 82.52%\n",
      "2-FCNN\n",
      "Trial 1\n",
      "Epochs: 2000/2000 - Train loss: 22.79%, accuracy: 97.75% - Validation loss: 36.19%, accuracy: 84.85%\n",
      "Test loss: 35.36%, accuracy: 86.5%\n",
      "Trial 2\n",
      "Epochs: 2000/2000 - Train loss: 26.37%, accuracy: 100.0% - Validation loss: 29.49%, accuracy: 93.94%\n",
      "Test loss: 33.57%, accuracy: 89.88%\n",
      "Trial 3\n",
      "Epochs: 2000/2000 - Train loss: 25.51%, accuracy: 100.0% - Validation loss: 31.14%, accuracy: 87.88%\n",
      "Test loss: 35.31%, accuracy: 88.34%\n",
      "Trial 4\n",
      "Epochs: 2000/2000 - Train loss: 27.39%, accuracy: 97.75% - Validation loss: 37.64%, accuracy: 84.85%\n",
      "Test loss: 32.82%, accuracy: 90.8%\n",
      "Trial 5\n",
      "Epochs: 2000/2000 - Train loss: 22.71%, accuracy: 96.63% - Validation loss: 33.73%, accuracy: 84.85%\n",
      "Test loss: 40.52%, accuracy: 83.44%\n",
      "Trial 6\n",
      "Epochs: 2000/2000 - Train loss: 27.78%, accuracy: 95.51% - Validation loss: 33.95%, accuracy: 93.94%\n",
      "Test loss: 39.49%, accuracy: 83.74%\n",
      "Trial 7\n",
      "Epochs: 2000/2000 - Train loss: 32.16%, accuracy: 95.51% - Validation loss: 36.47%, accuracy: 93.75%\n",
      "Test loss: 38.62%, accuracy: 87.12%\n",
      "Trial 8\n",
      "Epochs: 2000/2000 - Train loss: 26.6%, accuracy: 97.75% - Validation loss: 38.24%, accuracy: 84.38%\n",
      "Test loss: 36.55%, accuracy: 89.26%\n",
      "Trial 9\n",
      "Epochs: 2000/2000 - Train loss: 27.53%, accuracy: 98.86% - Validation loss: 25.4%, accuracy: 96.88%\n",
      "Test loss: 35.14%, accuracy: 88.96%\n",
      "Trial 10\n",
      "Epochs: 2000/2000 - Train loss: 26.01%, accuracy: 95.45% - Validation loss: 27.36%, accuracy: 96.88%\n",
      "Test loss: 38.12%, accuracy: 86.2%\n",
      "4-FCNN\n",
      "Trial 1\n",
      "Epochs: 2000/2000 - Train loss: 13.58%, accuracy: 98.88% - Validation loss: 30.3%, accuracy: 90.91%\n",
      "Test loss: 29.27%, accuracy: 87.73%\n",
      "Trial 2\n",
      "Epochs: 2000/2000 - Train loss: 15.61%, accuracy: 100.0% - Validation loss: 21.48%, accuracy: 93.94%\n",
      "Test loss: 26.08%, accuracy: 90.49%\n",
      "Trial 3\n",
      "Epochs: 2000/2000 - Train loss: 16.43%, accuracy: 100.0% - Validation loss: 22.71%, accuracy: 93.94%\n",
      "Test loss: 28.9%, accuracy: 88.65%\n",
      "Trial 4\n",
      "Epochs: 2000/2000 - Train loss: 16.89%, accuracy: 98.88% - Validation loss: 30.01%, accuracy: 87.88%\n",
      "Test loss: 26.26%, accuracy: 92.02%\n",
      "Trial 5\n",
      "Epochs: 2000/2000 - Train loss: 13.23%, accuracy: 98.88% - Validation loss: 28.16%, accuracy: 87.88%\n",
      "Test loss: 35.57%, accuracy: 84.97%\n",
      "Trial 6\n",
      "Epochs: 2000/2000 - Train loss: 17.37%, accuracy: 97.75% - Validation loss: 23.52%, accuracy: 93.94%\n",
      "Test loss: 31.04%, accuracy: 87.73%\n",
      "Trial 7\n",
      "Epochs: 2000/2000 - Train loss: 16.48%, accuracy: 96.63% - Validation loss: 24.1%, accuracy: 93.75%\n",
      "Test loss: 25.8%, accuracy: 90.49%\n",
      "Trial 8\n",
      "Epochs: 2000/2000 - Train loss: 14.67%, accuracy: 98.88% - Validation loss: 28.88%, accuracy: 87.5%\n",
      "Test loss: 28.22%, accuracy: 90.49%\n",
      "Trial 9\n",
      "Epochs: 2000/2000 - Train loss: 17.09%, accuracy: 98.86% - Validation loss: 14.95%, accuracy: 96.88%\n",
      "Test loss: 27.15%, accuracy: 90.8%\n",
      "Trial 10\n",
      "Epochs: 2000/2000 - Train loss: 19.36%, accuracy: 96.59% - Validation loss: 20.94%, accuracy: 96.88%\n",
      "Test loss: 32.92%, accuracy: 86.5%\n",
      "8-FCNN\n",
      "Trial 1\n",
      "Epochs: 2000/2000 - Train loss: 7.11%, accuracy: 100.0% - Validation loss: 26.92%, accuracy: 90.91%\n",
      "Test loss: 25.82%, accuracy: 89.57%\n",
      "Trial 2\n",
      "Epochs: 2000/2000 - Train loss: 8.37%, accuracy: 100.0% - Validation loss: 16.62%, accuracy: 93.94%\n",
      "Test loss: 22.29%, accuracy: 90.18%\n",
      "Trial 3\n",
      "Epochs: 2000/2000 - Train loss: 8.75%, accuracy: 100.0% - Validation loss: 16.41%, accuracy: 93.94%\n",
      "Test loss: 24.66%, accuracy: 90.49%\n",
      "Trial 4\n",
      "Epochs: 2000/2000 - Train loss: 9.45%, accuracy: 98.88% - Validation loss: 25.63%, accuracy: 90.91%\n",
      "Test loss: 23.64%, accuracy: 90.49%\n",
      "Trial 5\n",
      "Epochs: 2000/2000 - Train loss: 7.7%, accuracy: 100.0% - Validation loss: 26.79%, accuracy: 90.91%\n",
      "Test loss: 33.75%, accuracy: 85.58%\n",
      "Trial 6\n",
      "Epochs: 2000/2000 - Train loss: 9.2%, accuracy: 98.88% - Validation loss: 16.8%, accuracy: 93.94%\n",
      "Test loss: 26.01%, accuracy: 90.18%\n",
      "Trial 7\n",
      "Epochs: 2000/2000 - Train loss: 9.66%, accuracy: 100.0% - Validation loss: 21.06%, accuracy: 93.75%\n",
      "Test loss: 21.32%, accuracy: 91.1%\n",
      "Trial 8\n",
      "Epochs: 2000/2000 - Train loss: 7.69%, accuracy: 100.0% - Validation loss: 25.86%, accuracy: 84.38%\n",
      "Test loss: 24.54%, accuracy: 91.41%\n",
      "Trial 9\n",
      "Epochs: 2000/2000 - Train loss: 9.82%, accuracy: 100.0% - Validation loss: 10.0%, accuracy: 96.88%\n",
      "Test loss: 24.0%, accuracy: 91.72%\n",
      "Trial 10\n",
      "Epochs: 2000/2000 - Train loss: 8.97%, accuracy: 98.86% - Validation loss: 11.43%, accuracy: 96.88%\n",
      "Test loss: 25.85%, accuracy: 90.49%\n",
      "16-FCNN\n",
      "Trial 1\n",
      "Epochs: 2000/2000 - Train loss: 3.48%, accuracy: 100.0% - Validation loss: 26.51%, accuracy: 90.91%\n",
      "Test loss: 25.12%, accuracy: 90.18%\n",
      "Trial 2\n",
      "Epochs: 2000/2000 - Train loss: 4.02%, accuracy: 100.0% - Validation loss: 13.59%, accuracy: 96.97%\n",
      "Test loss: 21.25%, accuracy: 90.49%\n",
      "Trial 3\n",
      "Epochs: 2000/2000 - Train loss: 3.4%, accuracy: 100.0% - Validation loss: 12.99%, accuracy: 93.94%\n",
      "Test loss: 23.77%, accuracy: 90.49%\n",
      "Trial 4\n",
      "Epochs: 2000/2000 - Train loss: 4.72%, accuracy: 100.0% - Validation loss: 25.11%, accuracy: 87.88%\n",
      "Test loss: 24.62%, accuracy: 89.88%\n",
      "Trial 5\n",
      "Epochs: 2000/2000 - Train loss: 3.92%, accuracy: 100.0% - Validation loss: 29.05%, accuracy: 93.94%\n",
      "Test loss: 34.76%, accuracy: 85.89%\n",
      "Trial 6\n",
      "Epochs: 2000/2000 - Train loss: 4.6%, accuracy: 100.0% - Validation loss: 14.18%, accuracy: 96.97%\n",
      "Test loss: 25.4%, accuracy: 90.49%\n",
      "Trial 7\n",
      "Epochs: 2000/2000 - Train loss: 4.73%, accuracy: 100.0% - Validation loss: 21.81%, accuracy: 90.62%\n",
      "Test loss: 19.38%, accuracy: 91.41%\n",
      "Trial 8\n",
      "Epochs: 2000/2000 - Train loss: 3.59%, accuracy: 100.0% - Validation loss: 26.39%, accuracy: 84.38%\n",
      "Test loss: 23.14%, accuracy: 91.41%\n",
      "Trial 9\n",
      "Epochs: 2000/2000 - Train loss: 5.16%, accuracy: 100.0% - Validation loss: 7.38%, accuracy: 96.88%\n",
      "Test loss: 24.33%, accuracy: 91.72%\n",
      "Trial 10\n",
      "Epochs: 2000/2000 - Train loss: 5.49%, accuracy: 100.0% - Validation loss: 8.5%, accuracy: 96.88%\n",
      "Test loss: 24.43%, accuracy: 91.41%\n",
      "32-FCNN\n",
      "Trial 1\n",
      "Epochs: 2000/2000 - Train loss: 1.29%, accuracy: 100.0% - Validation loss: 28.86%, accuracy: 90.91%\n",
      "Test loss: 26.72%, accuracy: 89.57%\n",
      "Trial 2\n",
      "Epochs: 2000/2000 - Train loss: 1.75%, accuracy: 100.0% - Validation loss: 12.21%, accuracy: 93.94%\n",
      "Test loss: 22.36%, accuracy: 89.88%\n",
      "Trial 3\n",
      "Epochs: 2000/2000 - Train loss: 1.62%, accuracy: 100.0% - Validation loss: 12.59%, accuracy: 93.94%\n",
      "Test loss: 25.03%, accuracy: 91.72%\n",
      "Trial 4\n",
      "Epochs: 2000/2000 - Train loss: 2.31%, accuracy: 100.0% - Validation loss: 25.67%, accuracy: 87.88%\n",
      "Test loss: 27.22%, accuracy: 89.57%\n",
      "Trial 5\n",
      "Epochs: 2000/2000 - Train loss: 1.91%, accuracy: 100.0% - Validation loss: 33.78%, accuracy: 93.94%\n",
      "Test loss: 38.01%, accuracy: 86.5%\n",
      "Trial 6\n",
      "Epochs: 2000/2000 - Train loss: 2.14%, accuracy: 100.0% - Validation loss: 13.73%, accuracy: 96.97%\n",
      "Test loss: 27.53%, accuracy: 90.49%\n",
      "Trial 7\n",
      "Epochs: 2000/2000 - Train loss: 2.15%, accuracy: 100.0% - Validation loss: 25.05%, accuracy: 90.62%\n",
      "Test loss: 19.93%, accuracy: 90.8%\n",
      "Trial 8\n",
      "Epochs: 2000/2000 - Train loss: 1.54%, accuracy: 100.0% - Validation loss: 29.51%, accuracy: 87.5%\n",
      "Test loss: 23.42%, accuracy: 91.41%\n",
      "Trial 9\n",
      "Epochs: 2000/2000 - Train loss: 2.63%, accuracy: 100.0% - Validation loss: 6.22%, accuracy: 96.88%\n",
      "Test loss: 26.94%, accuracy: 90.49%\n",
      "Trial 10\n",
      "Epochs: 2000/2000 - Train loss: 2.28%, accuracy: 100.0% - Validation loss: 6.49%, accuracy: 96.88%\n",
      "Test loss: 24.99%, accuracy: 91.41%\n",
      "Dataset: cifar10 / Pair: 3-5\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Eseguire analisi statistica per confronto con Jones&Kording, Table 2, righe n-FCNN\n",
    "print(history)"
   ],
   "metadata": {
    "id": "7St40wVIR06V",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1647890584562,
     "user_tz": -60,
     "elapsed": 23,
     "user": {
      "displayName": "Lorenzo Petrella",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15951970565465703880"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e7a3b1fc-ef07-4703-b3df-6f0f617c2b22"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[[0.2609567  0.94532073]\n",
      "   [0.15470001 0.94689798]\n",
      "   [0.15390344 0.94794953]\n",
      "   [0.15529346 0.95425868]\n",
      "   [0.14052084 0.95636171]\n",
      "   [0.12704054 0.96161938]]\n",
      "\n",
      "  [[0.15896896 0.9584648 ]\n",
      "   [0.11082867 0.95268136]\n",
      "   [0.11478651 0.95268136]\n",
      "   [0.10812325 0.95951629]\n",
      "   [0.09437659 0.96792847]\n",
      "   [0.08930919 0.96898001]]\n",
      "\n",
      "  [[0.12778383 0.9584648 ]\n",
      "   [0.1328606  0.95478445]\n",
      "   [0.12545343 0.96004206]\n",
      "   [0.10370782 0.96582544]\n",
      "   [0.10322416 0.96687698]\n",
      "   [0.0897609  0.96950579]]\n",
      "\n",
      "  [[0.10260254 0.96056783]\n",
      "   [0.10496873 0.9584648 ]\n",
      "   [0.09170153 0.96319664]\n",
      "   [0.09293742 0.96214509]\n",
      "   [0.08273209 0.96950579]\n",
      "   [0.07715058 0.97213459]]\n",
      "\n",
      "  [[0.13745581 0.95162988]\n",
      "   [0.14262661 0.9511041 ]\n",
      "   [0.1281227  0.95320714]\n",
      "   [0.14441228 0.95057833]\n",
      "   [0.13990891 0.95373291]\n",
      "   [0.12807661 0.96529967]]\n",
      "\n",
      "  [[0.21488567 0.94164038]\n",
      "   [0.16266315 0.95951629]\n",
      "   [0.14738387 0.94637221]\n",
      "   [0.12947215 0.95425868]\n",
      "   [0.12344126 0.9584648 ]\n",
      "   [0.11539809 0.96740276]]\n",
      "\n",
      "  [[0.11075818 0.9584648 ]\n",
      "   [0.10667954 0.96004206]\n",
      "   [0.10397246 0.96214509]\n",
      "   [0.11117407 0.96004206]\n",
      "   [0.0806559  0.97371191]\n",
      "   [0.08992636 0.97108305]]\n",
      "\n",
      "  [[0.12346682 0.95478445]\n",
      "   [0.12704685 0.95162988]\n",
      "   [0.12701464 0.95478445]\n",
      "   [0.12995504 0.95636171]\n",
      "   [0.12350259 0.96319664]\n",
      "   [0.1112541  0.96635121]]\n",
      "\n",
      "  [[0.116445   0.95373291]\n",
      "   [0.11516548 0.95268136]\n",
      "   [0.10707746 0.95741326]\n",
      "   [0.10800562 0.95899051]\n",
      "   [0.09426254 0.96687698]\n",
      "   [0.07849047 0.97318614]]\n",
      "\n",
      "  [[0.1397137  0.95425868]\n",
      "   [0.12963389 0.95688748]\n",
      "   [0.14446157 0.95425868]\n",
      "   [0.15319808 0.95425868]\n",
      "   [0.1140777  0.96898001]\n",
      "   [0.12114415 0.97476339]]]\n",
      "\n",
      "\n",
      " [[[0.38812268 0.83249998]\n",
      "   [0.39885587 0.82999998]\n",
      "   [0.41675538 0.8265    ]\n",
      "   [0.40997466 0.83099997]\n",
      "   [0.43089333 0.83350003]\n",
      "   [0.4659386  0.83050001]]\n",
      "\n",
      "  [[0.45220843 0.83499998]\n",
      "   [0.37258065 0.83749998]\n",
      "   [0.37090924 0.83700001]\n",
      "   [0.36815768 0.83999997]\n",
      "   [0.36712295 0.84600002]\n",
      "   [0.3899335  0.84200001]]\n",
      "\n",
      "  [[0.4308421  0.83450001]\n",
      "   [0.41967389 0.83050001]\n",
      "   [0.43019694 0.82999998]\n",
      "   [0.41621327 0.82749999]\n",
      "   [0.46454856 0.83200002]\n",
      "   [0.47303492 0.82999998]]\n",
      "\n",
      "  [[0.44410869 0.83899999]\n",
      "   [0.36866045 0.83999997]\n",
      "   [0.37545568 0.84149998]\n",
      "   [0.37388948 0.83499998]\n",
      "   [0.38841918 0.83649999]\n",
      "   [0.40625441 0.83600003]]\n",
      "\n",
      "  [[0.4611575  0.82749999]\n",
      "   [0.40390122 0.829     ]\n",
      "   [0.42182291 0.82300001]\n",
      "   [0.4522413  0.824     ]\n",
      "   [0.46623775 0.81999999]\n",
      "   [0.48748499 0.8265    ]]\n",
      "\n",
      "  [[0.44909507 0.83249998]\n",
      "   [0.39736784 0.82849997]\n",
      "   [0.4071171  0.83099997]\n",
      "   [0.43877944 0.83149999]\n",
      "   [0.46622255 0.83050001]\n",
      "   [0.51806009 0.8405    ]]\n",
      "\n",
      "  [[0.45653239 0.81999999]\n",
      "   [0.41529322 0.81800002]\n",
      "   [0.43645597 0.81199998]\n",
      "   [0.46874374 0.81550002]\n",
      "   [0.50830406 0.81849998]\n",
      "   [0.51213664 0.81699997]]\n",
      "\n",
      "  [[0.39468342 0.82700002]\n",
      "   [0.39840186 0.82599998]\n",
      "   [0.41242078 0.82599998]\n",
      "   [0.42656085 0.829     ]\n",
      "   [0.46913373 0.81950003]\n",
      "   [0.49144    0.81900001]]\n",
      "\n",
      "  [[0.40901574 0.8265    ]\n",
      "   [0.41462842 0.82499999]\n",
      "   [0.42349347 0.82499999]\n",
      "   [0.45457891 0.8265    ]\n",
      "   [0.46912673 0.83600003]\n",
      "   [0.46684346 0.829     ]]\n",
      "\n",
      "  [[0.45424417 0.83899999]\n",
      "   [0.41387528 0.82800001]\n",
      "   [0.41375458 0.82249999]\n",
      "   [0.42684144 0.82300001]\n",
      "   [0.45627901 0.82599998]\n",
      "   [0.45874771 0.83350003]]]\n",
      "\n",
      "\n",
      " [[[0.37289438 0.86049998]\n",
      "   [0.37782791 0.85900003]\n",
      "   [0.37643251 0.86000001]\n",
      "   [0.38477227 0.87      ]\n",
      "   [0.36841065 0.88249999]\n",
      "   [0.36866963 0.88849998]]\n",
      "\n",
      "  [[0.50575602 0.87800002]\n",
      "   [0.36189258 0.87050003]\n",
      "   [0.39438626 0.86500001]\n",
      "   [0.38936096 0.87349999]\n",
      "   [0.34045842 0.89399999]\n",
      "   [0.36285231 0.89249998]]\n",
      "\n",
      "  [[0.40220779 0.87      ]\n",
      "   [0.36565253 0.8545    ]\n",
      "   [0.34136784 0.88200003]\n",
      "   [0.3428573  0.87550002]\n",
      "   [0.31649795 0.89300001]\n",
      "   [0.33110917 0.89249998]]\n",
      "\n",
      "  [[0.37416261 0.8725    ]\n",
      "   [0.28406507 0.88950002]\n",
      "   [0.28939626 0.89249998]\n",
      "   [0.32697988 0.88050002]\n",
      "   [0.29875031 0.903     ]\n",
      "   [0.28032407 0.90600002]]\n",
      "\n",
      "  [[0.34337077 0.86699998]\n",
      "   [0.35375547 0.86849999]\n",
      "   [0.34529194 0.88      ]\n",
      "   [0.39256015 0.87199998]\n",
      "   [0.37399679 0.88300002]\n",
      "   [0.35611963 0.89399999]]\n",
      "\n",
      "  [[0.32469866 0.85600001]\n",
      "   [0.33098024 0.86500001]\n",
      "   [0.35583723 0.85949999]\n",
      "   [0.27574232 0.90799999]\n",
      "   [0.28637955 0.90750003]\n",
      "   [0.26475322 0.91549999]]\n",
      "\n",
      "  [[0.4399111  0.8635    ]\n",
      "   [0.32201129 0.898     ]\n",
      "   [0.28650638 0.90700001]\n",
      "   [0.32075763 0.8915    ]\n",
      "   [0.30660731 0.90899998]\n",
      "   [0.29277566 0.91649997]]\n",
      "\n",
      "  [[0.40250114 0.86849999]\n",
      "   [0.36828145 0.87949997]\n",
      "   [0.34986368 0.89249998]\n",
      "   [0.40814507 0.89300001]\n",
      "   [0.40662688 0.89050001]\n",
      "   [0.3976042  0.89899999]]\n",
      "\n",
      "  [[0.35227063 0.87400001]\n",
      "   [0.35413203 0.89649999]\n",
      "   [0.35815555 0.889     ]\n",
      "   [0.34078342 0.89899999]\n",
      "   [0.36269209 0.90149999]\n",
      "   [0.36259851 0.90399998]]\n",
      "\n",
      "  [[0.51213205 0.86650002]\n",
      "   [0.33370814 0.88249999]\n",
      "   [0.42205676 0.84750003]\n",
      "   [0.42863497 0.88249999]\n",
      "   [0.39003539 0.8865    ]\n",
      "   [0.42164248 0.88550001]]]\n",
      "\n",
      "\n",
      " [[[0.05173654 0.98177844]\n",
      "   [0.05134202 0.97959185]\n",
      "   [0.05021838 0.98177844]\n",
      "   [0.04353607 0.98469388]\n",
      "   [0.05079801 0.98250729]\n",
      "   [0.05472193 0.98104954]]\n",
      "\n",
      "  [[0.06509493 0.98396504]\n",
      "   [0.06039212 0.98250729]\n",
      "   [0.07383304 0.98177844]\n",
      "   [0.06186495 0.98469388]\n",
      "   [0.07454185 0.98542273]\n",
      "   [0.03941364 0.98760933]]\n",
      "\n",
      "  [[0.07833809 0.97448981]\n",
      "   [0.05518472 0.978863  ]\n",
      "   [0.05585399 0.978863  ]\n",
      "   [0.05602374 0.97959185]\n",
      "   [0.05776672 0.97959185]\n",
      "   [0.05963918 0.9781341 ]]\n",
      "\n",
      "  [[0.09288411 0.98396504]\n",
      "   [0.04327342 0.98396504]\n",
      "   [0.04276489 0.98177844]\n",
      "   [0.04114595 0.98323613]\n",
      "   [0.0395759  0.98396504]\n",
      "   [0.04050561 0.98323613]]\n",
      "\n",
      "  [[0.08080776 0.97521865]\n",
      "   [0.05561376 0.98104954]\n",
      "   [0.05682141 0.98104954]\n",
      "   [0.05125993 0.98250729]\n",
      "   [0.05105294 0.98177844]\n",
      "   [0.04927449 0.98323613]]\n",
      "\n",
      "  [[0.06412213 0.98177844]\n",
      "   [0.06779684 0.98323613]\n",
      "   [0.06891451 0.98177844]\n",
      "   [0.06987772 0.98177844]\n",
      "   [0.04845389 0.98760933]\n",
      "   [0.06273872 0.98396504]]\n",
      "\n",
      "  [[0.04810759 0.98396504]\n",
      "   [0.05464926 0.98250729]\n",
      "   [0.04613344 0.98396504]\n",
      "   [0.05533486 0.98615158]\n",
      "   [0.03746552 0.98615158]\n",
      "   [0.05164659 0.98906708]]\n",
      "\n",
      "  [[0.04603727 0.98177844]\n",
      "   [0.04448709 0.98323613]\n",
      "   [0.04287026 0.98323613]\n",
      "   [0.03964715 0.98688048]\n",
      "   [0.041998   0.98396504]\n",
      "   [0.04544146 0.98542273]]\n",
      "\n",
      "  [[0.14455937 0.9759475 ]\n",
      "   [0.04800215 0.98250729]\n",
      "   [0.04635707 0.98250729]\n",
      "   [0.03624999 0.98760933]\n",
      "   [0.03635076 0.98615158]\n",
      "   [0.03897806 0.98833817]]\n",
      "\n",
      "  [[0.13095656 0.97230321]\n",
      "   [0.16723481 0.97230321]\n",
      "   [0.06160906 0.9766764 ]\n",
      "   [0.06201417 0.9759475 ]\n",
      "   [0.05688989 0.97959185]\n",
      "   [0.05951019 0.98104954]]]\n",
      "\n",
      "\n",
      " [[[0.69897622 0.57555604]\n",
      "   [0.68964982 0.54666358]\n",
      "   [0.71262771 0.54253614]\n",
      "   [0.68987018 0.54666358]\n",
      "   [0.71214694 0.55789953]\n",
      "   [0.69432312 0.55950469]]\n",
      "\n",
      "  [[0.68878472 0.54666358]\n",
      "   [0.70701665 0.56890619]\n",
      "   [0.70538425 0.52831918]\n",
      "   [0.5677281  0.7982114 ]\n",
      "   [0.72102481 0.52969503]\n",
      "   [0.72365505 0.52854848]]\n",
      "\n",
      "  [[0.69749737 0.60949326]\n",
      "   [0.75844371 0.50378352]\n",
      "   [0.71719104 0.64365971]\n",
      "   [0.62731963 0.73606968]\n",
      "   [0.69819427 0.53749138]\n",
      "   [0.70100969 0.51891768]]\n",
      "\n",
      "  [[0.69969261 0.603302  ]\n",
      "   [0.6910792  0.54666358]\n",
      "   [0.69111139 0.54689294]\n",
      "   [0.69218618 0.71451503]\n",
      "   [0.49956551 0.79568905]\n",
      "   [0.73201531 0.52946573]]\n",
      "\n",
      "  [[0.69025636 0.54666358]\n",
      "   [0.69025844 0.54666358]\n",
      "   [0.62732613 0.69364822]\n",
      "   [0.69053787 0.54666358]\n",
      "   [0.70916229 0.52854848]\n",
      "   [0.49111503 0.79591835]]\n",
      "\n",
      "  [[0.69077051 0.54666358]\n",
      "   [0.6905973  0.54666358]\n",
      "   [0.69065064 0.54643428]\n",
      "   [0.67739099 0.69525337]\n",
      "   [0.57596046 0.7924788 ]\n",
      "   [0.50760728 0.80738366]]\n",
      "\n",
      "  [[0.67711818 0.54001373]\n",
      "   [0.68869036 0.54574639]\n",
      "   [0.71763515 0.53061223]\n",
      "   [0.71461946 0.53130019]\n",
      "   [0.6356101  0.71841323]\n",
      "   [0.69001138 0.54666358]]\n",
      "\n",
      "  [[0.71919292 0.71336848]\n",
      "   [0.68947655 0.54666358]\n",
      "   [0.74104393 0.50745243]\n",
      "   [0.62907702 0.77252924]\n",
      "   [0.61129439 0.74524194]\n",
      "   [0.75170898 0.50401282]]\n",
      "\n",
      "  [[0.68972045 0.54666358]\n",
      "   [0.68939602 0.54666358]\n",
      "   [0.69052875 0.54551709]\n",
      "   [0.68993324 0.54666358]\n",
      "   [0.70611471 0.53336388]\n",
      "   [0.71192908 0.53107083]]\n",
      "\n",
      "  [[0.68938702 0.54666358]\n",
      "   [0.68986928 0.54666358]\n",
      "   [0.69726151 0.53198808]\n",
      "   [0.71367908 0.53703278]\n",
      "   [0.50273472 0.79018575]\n",
      "   [0.70972675 0.53244668]]]\n",
      "\n",
      "\n",
      " [[[0.4472329  0.83435583]\n",
      "   [0.35360375 0.86503065]\n",
      "   [0.29270273 0.87730062]\n",
      "   [0.25817963 0.89570552]\n",
      "   [0.25116125 0.90184051]\n",
      "   [0.26719841 0.89570552]]\n",
      "\n",
      "  [[0.43304285 0.88957053]\n",
      "   [0.33570939 0.89877301]\n",
      "   [0.26079768 0.904908  ]\n",
      "   [0.22288345 0.90184051]\n",
      "   [0.21249406 0.904908  ]\n",
      "   [0.22363773 0.89877301]]\n",
      "\n",
      "  [[0.45519331 0.86196321]\n",
      "   [0.35310641 0.88343561]\n",
      "   [0.28898978 0.88650304]\n",
      "   [0.24657847 0.904908  ]\n",
      "   [0.23771307 0.904908  ]\n",
      "   [0.25028387 0.91717792]]\n",
      "\n",
      "  [[0.43228027 0.89570552]\n",
      "   [0.32820082 0.90797544]\n",
      "   [0.26260668 0.92024541]\n",
      "   [0.23639014 0.904908  ]\n",
      "   [0.24617112 0.89877301]\n",
      "   [0.2722002  0.89570552]]\n",
      "\n",
      "  [[0.48422801 0.79447854]\n",
      "   [0.40524942 0.83435583]\n",
      "   [0.35569236 0.84969324]\n",
      "   [0.33748719 0.85582823]\n",
      "   [0.34756273 0.85889572]\n",
      "   [0.3801111  0.86503065]]\n",
      "\n",
      "  [[0.50098789 0.76687115]\n",
      "   [0.39490327 0.83742332]\n",
      "   [0.31041822 0.87730062]\n",
      "   [0.26011795 0.90184051]\n",
      "   [0.25404742 0.904908  ]\n",
      "   [0.27528822 0.904908  ]]\n",
      "\n",
      "  [[0.54119414 0.84355831]\n",
      "   [0.38621882 0.87116563]\n",
      "   [0.25797716 0.904908  ]\n",
      "   [0.21320269 0.91104293]\n",
      "   [0.19384745 0.91411042]\n",
      "   [0.19932432 0.90797544]]\n",
      "\n",
      "  [[0.45988426 0.88036811]\n",
      "   [0.36552981 0.89263803]\n",
      "   [0.28221041 0.904908  ]\n",
      "   [0.24541351 0.91411042]\n",
      "   [0.23144779 0.91411042]\n",
      "   [0.23417397 0.91411042]]\n",
      "\n",
      "  [[0.43558994 0.88957053]\n",
      "   [0.35139486 0.88957053]\n",
      "   [0.27146131 0.90797544]\n",
      "   [0.23998064 0.91717792]\n",
      "   [0.24327902 0.91717792]\n",
      "   [0.26940107 0.904908  ]]\n",
      "\n",
      "  [[0.46627563 0.82515335]\n",
      "   [0.38117406 0.86196321]\n",
      "   [0.32923332 0.86503065]\n",
      "   [0.2585398  0.904908  ]\n",
      "   [0.24425323 0.91411042]\n",
      "   [0.24985968 0.91411042]]]\n",
      "\n",
      "\n",
      " [[[0.7425403  0.56950003]\n",
      "   [0.72298056 0.5715    ]\n",
      "   [0.7037524  0.57499999]\n",
      "   [0.68841404 0.50550002]\n",
      "   [0.71019882 0.56300002]\n",
      "   [0.73687011 0.5715    ]]\n",
      "\n",
      "  [[0.69359791 0.5       ]\n",
      "   [0.75023651 0.56150001]\n",
      "   [0.71171755 0.56550002]\n",
      "   [0.69267696 0.5       ]\n",
      "   [0.78247064 0.56400001]\n",
      "   [0.77486736 0.56650001]]\n",
      "\n",
      "  [[0.69267839 0.5       ]\n",
      "   [0.69338405 0.49849999]\n",
      "   [0.76093936 0.56849998]\n",
      "   [0.72075254 0.56400001]\n",
      "   [0.71275991 0.56449997]\n",
      "   [0.70792794 0.5625    ]]\n",
      "\n",
      "  [[0.69023031 0.50950003]\n",
      "   [0.71855742 0.57599998]\n",
      "   [0.72655553 0.57450002]\n",
      "   [0.74199879 0.56449997]\n",
      "   [0.73251051 0.55650002]\n",
      "   [0.76419628 0.56349999]]\n",
      "\n",
      "  [[0.69921458 0.56      ]\n",
      "   [0.69239402 0.53399998]\n",
      "   [0.719374   0.56900001]\n",
      "   [0.76957357 0.57950002]\n",
      "   [0.73443657 0.56      ]\n",
      "   [0.73273557 0.56599998]]\n",
      "\n",
      "  [[0.70894009 0.57700002]\n",
      "   [0.73958087 0.5625    ]\n",
      "   [0.71026731 0.5625    ]\n",
      "   [0.70748121 0.5625    ]\n",
      "   [0.7360791  0.56199998]\n",
      "   [0.74407828 0.55849999]]\n",
      "\n",
      "  [[0.67607844 0.58249998]\n",
      "   [0.67766744 0.58099997]\n",
      "   [0.6833322  0.58649999]\n",
      "   [0.6961925  0.5995    ]\n",
      "   [0.71718949 0.59249997]\n",
      "   [0.67317152 0.58950001]]\n",
      "\n",
      "  [[0.74152666 0.56699997]\n",
      "   [0.71341395 0.56099999]\n",
      "   [0.73236454 0.58050001]\n",
      "   [0.70248282 0.57099998]\n",
      "   [0.71984512 0.565     ]\n",
      "   [0.7420364  0.5715    ]]\n",
      "\n",
      "  [[0.6925869  0.50050002]\n",
      "   [0.69200087 0.5       ]\n",
      "   [0.74323815 0.58200002]\n",
      "   [0.73309374 0.588     ]\n",
      "   [0.72509825 0.5855    ]\n",
      "   [0.6937232  0.579     ]]\n",
      "\n",
      "  [[0.69133562 0.57550001]\n",
      "   [0.73740834 0.56349999]\n",
      "   [0.73417431 0.5625    ]\n",
      "   [0.72093141 0.55900002]\n",
      "   [0.72806877 0.55849999]\n",
      "   [0.71366549 0.56349999]]]]\n"
     ]
    }
   ]
  }
 ]
}