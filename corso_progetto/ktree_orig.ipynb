{"cells":[{"cell_type":"markdown","metadata":{"id":"lFrTiHVXnp5S"},"source":["# **Sezione riservata a Google Colab**"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"R1TynGfJn9FM","executionInfo":{"status":"ok","timestamp":1647940997985,"user_tz":-60,"elapsed":242,"user":{"displayName":"Lorenzo Petrella","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15951970565465703880"}}},"outputs":[],"source":["project_folder = './drive/MyDrive/Colab_Notebooks/Progetto_Finale/'"]},{"cell_type":"markdown","metadata":{"id":"42Wi-Zmun_iZ"},"source":["# **Libreria**"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"tGQFnZlg-tSn","executionInfo":{"status":"ok","timestamp":1647941003025,"user_tz":-60,"elapsed":4439,"user":{"displayName":"Lorenzo Petrella","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15951970565465703880"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","\n","class ImageDataset:\n","\n","    def __init__(self, ds_name, train_test, shuffle_files=True, data_dir=\"./data\", USPS_dir=\"./USPS/\"):\n","\n","        if ds_name == 'mnist':\n","\n","            ds = tfds.load('mnist', split=train_test, shuffle_files=shuffle_files,\n","                                download=True, data_dir=data_dir, as_supervised=True, batch_size=-1)\n","\n","        elif ds_name == 'fmnist':\n","\n","            ds = tfds.load('fashion_mnist', split=train_test, shuffle_files=shuffle_files,\n","                                download=True, data_dir=data_dir, as_supervised=True, batch_size=-1)\n","\n","        elif ds_name == 'cifar10':\n","\n","            ds = tfds.load('cifar10', split=train_test, shuffle_files=shuffle_files,\n","                                download=True, data_dir=data_dir, as_supervised=True, batch_size=-1)\n","\n","        elif ds_name == 'kmnist':\n","\n","            ds = tfds.load('kmnist', split=train_test, shuffle_files=shuffle_files,\n","                                download=True, data_dir=data_dir, as_supervised=True, batch_size=-1)\n","\n","        elif ds_name == 'emnist':\n","\n","            ds = tfds.load('emnist', split=train_test, shuffle_files=shuffle_files,\n","                                download=True, data_dir=data_dir, as_supervised=True, batch_size=-1)\n","\n","        elif ds_name == 'svhn':\n","\n","            ds = tfds.load('svhn', split=train_test, shuffle_files=shuffle_files,\n","                                download=True, data_dir=data_dir, as_supervised=True, batch_size=-1)\n","\n","        elif ds_name == 'usps':\n","\n","            self.images = np.load(USPS_dir + train_test + '_x.npy')\n","            self.labels = np.load(USPS_dir + train_test + '_y.npy')\n","\n","        else:\n","            raise Exception(\"Selected dataset is not available\")\n","\n","        if ds_name != 'usps':\n","            self.images, self.labels = tfds.as_numpy(ds)\n","\n","        self.num_channels = self.images.shape[3]\n","\n","    def shuffle(self):\n","\n","        p = np.random.permutation(len(self.images))\n","        self.images = self.images[p]\n","        self.labels = self.labels[p]\n","\n","\n","    def normalize(self, n_bits=8):\n","\n","        self.images = self.images / (2**n_bits - 1)\n","\n","    def filter(self, t1, t2=None, binary=True, overwrite=False):\n","\n","        images = self.images\n","        labels = self.labels\n","        target_1 = np.where(labels == t1)\n","        target_2 = np.where(labels == t2)\n","\n","        if t2 is not None:\n","            images = images[np.where((labels == t1) | (labels == t2))]\n","            labels = labels[np.where((labels == t1) | (labels == t2))]\n","\n","        target_1 = np.where(labels == t1)\n","        target_2 = np.where(labels == t2)\n","\n","        if binary:\n","            labels[target_1] = 1\n","            labels[target_2] = 0\n","\n","        if not overwrite:\n","          return images, labels\n","          \n","        else:\n","          self.images = images\n","          self.labels = labels\n","\n","    def pad(self, target_shape=(32, 32)):\n","        \"\"\"\n","        Adds symmetric zero padding to image with shape HxWxC\n","        :param target_shape: tuple with target shape (H,W)\n","        \"\"\"\n","\n","        self.images = np.pad(self.images, ((0,0),(2,2),(2,2),(0,0)))\n","\n","    def vectorize(self, merge_channels=False):\n","        \"\"\"\n","        Transforms image from pixel matrix with shape (H,W,C) to linear vector\n","        :param merge_channels: if True, the pixel vectors from each channel will be concatenated\n","        \"\"\"\n","\n","        self.images = np.transpose(self.images, (0, 3, 1, 2))\n","\n","        if merge_channels:\n","            self.images = np.reshape(self.images, [len(self.images), -1])\n","\n","        else:\n","            self.images = np.reshape(self.images, [len(self.images), self.images.shape[1], -1])\n","\n","    def permute(self, n):\n","\n","        perm_idx = get_permutation(n)\n","        if self.num_channels == 3:\n","            perm_idx = np.concatenate((perm_idx,perm_idx,perm_idx),0)\n","        self.images = self.images[:, perm_idx]\n","\n","    def subset(self, shard=False, shard_number=None, validation=False, validation_size=None, tile=None):\n","\n","        if validation and validation_size is None:\n","            raise Exception(\"Requested training/validation split but no validation split size given.\")\n","        if shard and shard_number is None:\n","            raise Exception(\"Requested sharding but no shard size given.\")\n","\n","        if validation:\n","            valid_images, valid_labels = self.images[0:validation_size], self.labels[0:validation_size]\n","            train_images, train_labels = self.images[validation_size:], self.labels[validation_size:]\n","            if tile is not None:\n","                valid_images = np.tile(valid_images, (1, tile))\n","                train_images = np.tile(train_images, (1, tile))\n","            if shard:\n","                valid_images, valid_labels = np.array_split(valid_images, shard_number), np.array_split(valid_labels, shard_number)\n","                train_images, train_labels = np.array_split(train_images, shard_number), np.array_split(train_labels, shard_number)\n","            return train_images, train_labels, valid_images, valid_labels\n","        else:\n","            train_images, train_labels = self.images, self.labels\n","            if tile is not None:\n","                train_images = np.tile(train_images, (1, tile))\n","            if shard:\n","                train_images, train_labels = np.array_split(train_images, shard_number), np.array_split(train_labels, shard_number)\n","            return train_images, train_labels\n","\n","\n","def get_matrix(n):\n","    '''\n","     Assumes that the matrix is of size 2^n x 2^n for some n\n","\n","     EXAMPLE for n=4\n","\n","     Old order:\n","\n","      1  2  3  4\n","      5  6  7  8\n","      9 10 11 12\n","     13 14 15 16\n","\n","     New order:\n","\n","      1  2  5  6\n","      3  4  7  8\n","      9 10 13 14\n","     11 12 15 16\n","\n","     Function returns numbers from old order, read in the order of the new numbers:\n","\n","     [1, 2, 5, 6, 3, 4, 7, 8, 9, 10, 13, 14, 11, 12, 15, 16]\n","\n","     So if you previously had a data vector v from a matrix size 32 x 32,\n","     you can now use v[get_permutation(5)] to reorder the elements.\n","    '''\n","    if n == 0:\n","        return np.array([[1]])\n","    else:\n","        smaller = get_matrix(n - 1)\n","        num_in_smaller = 2 ** (2 * n - 2)\n","        first_stack = np.hstack((smaller, smaller + num_in_smaller))\n","        return np.vstack((first_stack, first_stack + 2 * num_in_smaller))\n","\n","\n","def get_permutation(n):\n","    return get_matrix(n).ravel() - 1\n"]},{"cell_type":"markdown","metadata":{"id":"iZFxvzHy_pb6"},"source":["# **Body**"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"kxKDUPDCLEHY","executionInfo":{"status":"ok","timestamp":1647941003025,"user_tz":-60,"elapsed":4,"user":{"displayName":"Lorenzo Petrella","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15951970565465703880"}}},"outputs":[],"source":["import tensorflow as tf\n","\n","class treeLayer(tf.keras.layers.Layer):\n","\n","  def __init__(self, Input_size=3072, Activation=\"relu\"):\n","\n","    super(treeLayer, self).__init__()\n","    self.Input_size = Input_size\n","    self.Activation = Activation\n","\n","  def build(self, input_shape):\n","\n","    self.kernel = self.add_weight(shape=(1, self.Input_size),\n","                                  initializer=tf.keras.initializers.HeNormal,\n","                                  trainable=True)\n","    self.summer = np.zeros((self.Input_size, self.Input_size//2))\n","    for i in range(self.Input_size):\n","        self.summer[i, i//2] = 1\n","\n","    self.summer = tf.convert_to_tensor(self.summer, dtype=tf.float32)\n","      \n","  def call(self, inputs):  \n","    \n","    x = tf.math.multiply(inputs, self.kernel)\n","    x = tf.matmul(x, self.summer)\n","    x = tf.nn.leaky_relu(x, alpha = .01)\n","    return x\n","\n","def create_model(input_size, num_trees=1):\n","  model = tf.keras.Sequential()\n","  while input_size > num_trees:\n","    model.add(treeLayer(input_size))\n","    input_size = input_size//2\n","  model.add(tf.keras.layers.Dense(units=1, activation='sigmoid', use_bias=False))\n","\n","  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n","                  loss=tf.keras.losses.BinaryCrossentropy(\n","                      reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE\n","                  ),\n","                  metrics=['acc'])\n","  \n","  return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450,"referenced_widgets":["8893c8e692264c158d5ef02dfdbb24f9","e2223afc63bd440bb14785f72c9d33d2","3ec39dd1446f40a18b57b70ab718335f","5ffa722cc9b146bc80fa7014e6ce9e6a","9659bca9e8d841dd8e5d34af70e2f42c","9d2c9d285c12486aa67f5de8e4f2b312","5ea671b3acb2405e86cd004be6788104","d4e39cdd9e484363a9dce84758b81e59","b7c3ddc073084520a7c0c9d857e20787","9b54d9a81d1946af83998f93174a8821","d9ed125a730b492fb44b099a4f368e34"]},"id":"_p5yaGWI_rhQ","outputId":"d1211732-72bc-4e6e-97f9-0b2a031d15ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset: mnist / Pair: 3-5\n","\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to ./data/mnist/3.0.1...\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n","local data directory. If you'd instead prefer to read directly from our public\n","GCS bucket (recommended if you're running on GCP), you can instead pass\n","`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n","\n"]},{"output_type":"display_data","data":{"text/plain":["Dl Completed...:   0%|          | 0/4 [00:00<?, ? file/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8893c8e692264c158d5ef02dfdbb24f9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\u001b[1mDataset mnist downloaded and prepared to ./data/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py:598: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.get_single_element()`.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py:598: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.get_single_element()`.\n"]},{"output_type":"stream","name":"stdout","text":["1-FCNN\n","2-FCNN\n","4-FCNN\n","8-FCNN\n","16-FCNN\n","32-FCNN\n","Trial 1\n"]}],"source":["import gc\n","\n","# Initialize settings\n","bs = 256\n","trials = 10\n","epochs = 2000\n","trees_set = [1,2,4,8,16,32]\n","\n","# Load class-dataset list\n","# classes = np.load(project_folder + 'results/classes.npy', allow_pickle=True)\n","\n","classes = [[3, 5, 'mnist']]\n","\n","callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=60)\n","\n","history = np.load(project_folder + 'results/fcnn_history.npy', allow_pickle=True)\n","#history = np.zeros((len(classes), trials, len(trees_set), 2))\n","\n","# For each dataset enumerated from classes list\n","for j, (t1, t2, ds) in enumerate(classes):\n","\n","    print(f\"Dataset: {ds} / Pair: {t1}-{t2}\")\n","\n","    test_ds = ImageDataset(ds, 'test')\n","    train_ds = ImageDataset(ds, 'train')\n","\n","    for x in [train_ds, test_ds]:\n","        x.filter(t1, t2, overwrite=True)\n","        x.shuffle()\n","        x.normalize()\n","        if x.images.shape[1:3] == (28, 28):\n","            x.pad()\n","        x.vectorize(True)\n","    \n","\n","    for k, trees in enumerate(trees_set):\n","\n","      print(f\"{trees}-FCNN\")\n","      X_train, y_train, X_valid, y_valid = train_ds.subset(shard=True, shard_number=trials, validation=True,\n","                                                           validation_size=len(test_ds.images))\n","      X_test, y_test = test_ds.subset()\n","      test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test)).map(lambda x, y: (tf.tile(x, [trees]), y)).batch(bs)\n","\n","      if history[j, -1, k, 0] != 0:\n","        continue\n","\n","      for i in range(trials):\n","\n","        print(f\"Trial {i+1}\")\n","\n","        #with tf.device('/device:GPU:0'):\n","\n","        model = create_model(input_size=X_train[i].shape[1]*trees, num_trees=trees)\n","\n","        train_set = tf.data.Dataset.from_tensor_slices((X_train[i], y_train[i])).map(lambda x, y: (tf.tile(x, [trees]), y)).batch(bs)\n","        valid_set = tf.data.Dataset.from_tensor_slices((X_valid[i], y_valid[i])).map(lambda x, y: (tf.tile(x, [trees]), y)).batch(bs)\n","\n","        fit_history = model.fit(x = train_set, batch_size = bs, epochs = epochs,\n","                      validation_data = valid_set, validation_batch_size = bs,\n","                      callbacks=[callback], verbose=0)\n","                  \n","        n_epochs = len(fit_history.history['acc'])\n","        train_loss = round(fit_history.history['loss'][-1]*100, 2)\n","        train_acc = round(fit_history.history['acc'][-1]*100, 2)\n","        valid_loss = round(fit_history.history['val_loss'][-1]*100, 2)\n","        valid_acc = round(fit_history.history['val_acc'][-1]*100, 2)\n","        print(f\"Epochs: {n_epochs}/{epochs} - Train loss: {train_loss}%, accuracy: {train_acc}% - Validation loss: {valid_loss}%, accuracy: {valid_acc}%\")\n","\n","        evaluate_history = model.evaluate(x = test_set, batch_size = bs, verbose=0)\n","        print(f\"Test loss: {round(evaluate_history[0]*100, 2)}%, accuracy: {round(evaluate_history[1]*100, 2)}%\")\n","        \n","        history[j, i, k] = evaluate_history\n","\n","        np.save(project_folder+'results/fcnn_history.npy', history,\n","                allow_pickle=True)\n","        \n","        del model, train_set, valid_set\n","        gc.collect()"]},{"cell_type":"code","source":["print(history)\n","model.summary()"],"metadata":{"id":"4yfTGA7VxpD2"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"ktree_orig.ipynb","provenance":[{"file_id":"1yPa4_TERRnpMOCLciJNiJVyLEDfyy26l","timestamp":1647559041758}],"collapsed_sections":[],"mount_file_id":"1qIeM9anBGN7gxh7issfVzLlQPB1xsao0","authorship_tag":"ABX9TyPtoL+ouxkw4LpIUIYcN2kh"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8893c8e692264c158d5ef02dfdbb24f9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e2223afc63bd440bb14785f72c9d33d2","IPY_MODEL_3ec39dd1446f40a18b57b70ab718335f","IPY_MODEL_5ffa722cc9b146bc80fa7014e6ce9e6a"],"layout":"IPY_MODEL_9659bca9e8d841dd8e5d34af70e2f42c"}},"e2223afc63bd440bb14785f72c9d33d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d2c9d285c12486aa67f5de8e4f2b312","placeholder":"​","style":"IPY_MODEL_5ea671b3acb2405e86cd004be6788104","value":"Dl Completed...: 100%"}},"3ec39dd1446f40a18b57b70ab718335f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4e39cdd9e484363a9dce84758b81e59","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b7c3ddc073084520a7c0c9d857e20787","value":4}},"5ffa722cc9b146bc80fa7014e6ce9e6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b54d9a81d1946af83998f93174a8821","placeholder":"​","style":"IPY_MODEL_d9ed125a730b492fb44b099a4f368e34","value":" 4/4 [00:00&lt;00:00,  9.12 file/s]"}},"9659bca9e8d841dd8e5d34af70e2f42c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d2c9d285c12486aa67f5de8e4f2b312":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ea671b3acb2405e86cd004be6788104":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4e39cdd9e484363a9dce84758b81e59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7c3ddc073084520a7c0c9d857e20787":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9b54d9a81d1946af83998f93174a8821":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9ed125a730b492fb44b099a4f368e34":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}